#!/usr/bin/env python
from __future__ import print_function
import numpy as np
import sys
import os
import cv2

import ensure_segmappy_is_installed
from segmappy import Config
from segmappy import Dataset
from segmappy import Generator
from segmappy.tools.classifiertools import get_default_dataset, get_default_preprocessor
from segmappy.tools.roccurve import get_roc_pairs, get_roc_curve
from segmappy.models.model_groups_tf import init_model

# read config file
configfile = "default_training.ini"

config = Config(configfile)

# add command line arguments to config
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--log")
parser.add_argument("--debug", action="store_true")
parser.add_argument("--checkpoints", type=int, default=1)
parser.add_argument("--keep-best", action="store_true")
parser.add_argument("--roc", action="store_true")
parser.add_argument("--vis_views", action="store_true")
args = parser.parse_args()
config.log_name = args.log
config.debug = args.debug
config.checkpoints = args.checkpoints
config.keep_best = args.keep_best
config.roc = args.roc
config.vis_views = args.vis_views

# create or empty the model folder
if not os.path.exists(config.cnn_model_folder):
    os.makedirs(config.cnn_model_folder)
else:
    import glob

    model_files = glob.glob(os.path.join(config.cnn_model_folder, "*"))
    for model_file in model_files:
        os.remove(model_file)

# load preprocessor
preprocessor = get_default_preprocessor(config)

segments = []
classes = np.array([], dtype=np.int)
n_classes = 0
duplicate_classes = np.array([], dtype=np.int)
max_duplicate_class = 0
duplicate_ids = np.array([], dtype=np.int)
int_paths = []
mask_paths = []
range_paths = []

runs = config.cnn_train_folders.split(",")
for run in runs:
    dataset = get_default_dataset(config, run)

    [run_segments, _, run_classes, run_n_classes, _, _, _, run_int_paths, run_mask_paths,
     run_range_paths] = dataset.load(preprocessor=preprocessor)
    run_duplicate_classes = dataset.duplicate_classes
    run_duplicate_ids = dataset.duplicate_ids

    run_classes += n_classes
    run_duplicate_classes += max_duplicate_class

    segments += run_segments
    classes = np.concatenate((classes, run_classes), axis=0)
    n_classes += run_n_classes
    duplicate_classes = np.concatenate(
        (duplicate_classes, run_duplicate_classes), axis=0
    )
    duplicate_ids = np.concatenate((duplicate_ids, run_duplicate_ids), axis=0)
    int_paths += run_int_paths
    mask_paths += run_mask_paths
    range_paths += run_range_paths

    max_duplicate_class = np.max(duplicate_classes) + 1

# split so that the test set contains entire sequences
train_fold = np.zeros(classes.shape, dtype=np.int)

train_ids = np.where(train_fold == 1)[0]
test_ids = np.where(train_fold == 0)[0]

# initialize preprocessor
preprocessor.init_segments(segments,
                           classes,
                           train_ids=train_ids,
                           int_paths=int_paths,
                           mask_paths=mask_paths,
                           range_paths=range_paths)

# save scaler mean in a csv
if config.remove_mean:
    scaler_path = os.path.join(config.cnn_model_folder, "scaler_mean.csv")
    with open(scaler_path, "w") as fp:
        for i in preprocessor._scaler.mean_:
            fp.write(str(i) + "\n")

# save the scaler as well using pickle
if config.remove_mean or config.remove_std:
    scaler_path = os.path.join(config.cnn_model_folder, "scaler.pkl")
    preprocessor.save_scaler(scaler_path)

# initialize segment batch generators

gen_test = Generator(
    preprocessor,
    test_ids,
    n_classes,
    train=False,
    batch_size=config.batch_size,
    shuffle=True,
)

print("Testing with %d segments" % gen_test.n_segments)

import tensorflow as tf

tf.reset_default_graph()

# restore variable names from previous session
saver = tf.train.import_meta_graph(config.cnn_model_folder_w_vis_views + "/model.ckpt.meta")

# get key tensorflow variables
cnn_graph = tf.get_default_graph()

cnn_input = cnn_graph.get_tensor_by_name("InputScope/input:0")
cnn_input_vis = cnn_graph.get_tensor_by_name("InputScope/input_vis:0")
y_true = cnn_graph.get_tensor_by_name("y_true:0")
training = cnn_graph.get_tensor_by_name("training:0")
scales = cnn_graph.get_tensor_by_name("scales:0")

loss = cnn_graph.get_tensor_by_name("loss:0")
loss_c = cnn_graph.get_tensor_by_name("loss_c:0")
loss_r = cnn_graph.get_tensor_by_name("loss_r:0")

accuracy = cnn_graph.get_tensor_by_name("accuracy:0")
y_prob = cnn_graph.get_tensor_by_name("y_prob:0")
descriptor = cnn_graph.get_tensor_by_name("OutputScope/descriptor_read:0")
roc_auc = cnn_graph.get_tensor_by_name("roc_auc:0")

#visualization
conv_vis = cnn_graph.get_tensor_by_name("conv5_vis_out:0")
conv_vis_grad = cnn_graph.get_tensor_by_name("conv5_vis_grad_out:0")
img_heatmap = cnn_graph.get_tensor_by_name("img_heatmap:0")

global_step = cnn_graph.get_tensor_by_name("global_step:0")
update_step = cnn_graph.get_tensor_by_name("update_step:0")
train_op = cnn_graph.get_operation_by_name("train_op")

summary_batch = tf.summary.merge_all("summary_batch")
summary_epoch = tf.summary.merge_all("summary_epoch")
summary_heatmap = tf.summary.merge_all("summary_heatmap")

# os.environ['CUDA_VISIBLE_DEVICES'] = "1"

config_tf = tf.ConfigProto()
config_tf.gpu_options.allow_growth = True

with tf.Session(config=config_tf) as sess:
    # tensorboard statistics
    if config.log_name:
        test_writer = tf.summary.FileWriter(
            os.path.join(config.log_path, config.log_name, "test")
        )

    # initialize all tf variables
    tf.global_variables_initializer().run()

    saver.restore(sess, config.cnn_model_folder_w_vis_views + "/model.ckpt")

    # sequence of train and test batches
    batches = [0] * gen_test.n_batches
    for epoch in range(0, config.n_epochs):
        train_loss = 0
        train_loss_c = 0
        train_loss_r = 0
        train_accuracy = 0
        train_step = 0

        test_loss = 0
        test_loss_c = 0
        test_loss_r = 0
        test_accuracy = 0
        test_step = 0

        np.random.shuffle(batches)

        console_output_size = 0
        for step, train in enumerate(batches):
            batch_segments, batch_classes, batch_vis_views = gen_test.next()

            # calculate test loss and accuracy
            [summary, batch_loss, batch_loss_c, batch_loss_r,
             batch_accuracy, batch_prob, batch_conv_vis, batch_conv_vis_grad] = sess.run(
                [summary_batch, loss, loss_c, loss_r, accuracy, y_prob, conv_vis, conv_vis_grad],
                feed_dict={
                    cnn_input: batch_segments,
                    cnn_input_vis: batch_vis_views,
                    y_true: batch_classes,
                    scales: preprocessor.last_scales,
                },
            )

            if config.log_name:
                test_writer.add_summary(summary, sess.run(global_step))

            # if test_step == gen_test.n_batches - 1:
            if True:
                intensity = batch_vis_views[:, :, :, 0] * 173.09 + 209.30
                mask = batch_vis_views[:, :, :, 1] * 255.0
                heatmap = batch_conv_vis * np.sum(batch_conv_vis_grad, axis=(1, 2), keepdims=True) / (intensity.shape[1] * intensity.shape[2])
                heatmap = np.sum(heatmap, axis=-1)
                img_heatmap_np = np.zeros(intensity.shape + (3,), dtype=np.uint8)
                for b in range(heatmap.shape[0]):
                    cur_heatmap = cv2.resize(heatmap[b], (intensity.shape[2], intensity.shape[1]))
                    cur_heatmap = np.maximum(cur_heatmap, 0.0)
                    cur_heatmap = cur_heatmap / np.max(cur_heatmap)
                    cur_heatmap = cv2.applyColorMap(np.uint8(255 * cur_heatmap), cv2.COLORMAP_JET)
                    cur_img_heatmap = 0.5 * cur_heatmap + \
                                      np.tile(np.expand_dims(np.minimum(intensity[b] * 255.0 / 1500.0, 255.0), axis=-1),
                                              [1, 1, 3])
                    # cur_img_heatmap = np.tile(
                    #                       np.expand_dims(np.minimum(intensity[b] * 255.0 / 1500.0, 255.0), axis=-1),
                    #                       [1, 1, 3])
                    cur_img_heatmap[mask[b] > 128.0, :] = np.array([255, 0, 0], dtype=np.uint8)
                    img_heatmap_np[b] = cur_img_heatmap

                summary = sess.run(summary_heatmap, feed_dict={img_heatmap: img_heatmap_np})
                test_writer.add_summary(summary, sess.run(global_step))
                test_writer.flush()

            test_loss += batch_loss
            test_loss_c += batch_loss_c
            test_loss_r += batch_loss_r
            test_accuracy += batch_accuracy
            test_step += 1

            # print results
            sys.stdout.write("\b" * console_output_size)

            console_output = "epoch %2d, step %05d " % (epoch, step)

            if test_step:
                console_output += "v_loss %.4f v_acc %.2f v_c %.4f v_r %.4f" % (
                    test_loss / test_step,
                    test_accuracy / test_step * 100,
                    test_loss_c / test_step,
                    test_loss_r / test_step,
                )

            console_output_size = len(console_output)

            sys.stdout.write(console_output)
            sys.stdout.flush()

        # flush tensorboard log
        if config.log_name:
            test_writer.flush()

        # # save epoch model
        # if not config.keep_best or test_accuracy > best_accuracy:
        #     if config.checkpoints > 1:
        #         model_name = "model-%d.ckpt" % sess.run(global_step)
        #     else:
        #         model_name = "model.ckpt"
        #
        #     if config.vis_views:
        #         model_folder = config.cnn_model_folder_w_vis_views
        #     else:
        #         model_folder = config.cnn_model_folder_wo_vis_views
        #     saver.save(sess, os.path.join(model_folder, model_name))
        #     tf.train.write_graph(
        #         sess.graph.as_graph_def(), model_folder, "graph.pb"
        #     )

        print()
